# English-Telugu Translation with mBART Fine-tuning

_A production-ready implementation of English‚ÜîTelugu neural machine translation using mBART fine-tuning_

[Features](#features) ‚Ä¢ [Installation](#installation) ‚Ä¢ [Quick Start](#quick-start) ‚Ä¢ [Documentation](#documentation) ‚Ä¢ [Troubleshooting](#troubleshooting)

</div>

---

## üìñ Overview

This repository contains a complete pipeline for fine-tuning Facebook's mBART-50 multilingual model for English-Telugu translation. The implementation demonstrates how pre-trained transformer architectures can achieve **95%+ accuracy** on low-resource language pairs with minimal data.

### Key Results

| Metric            | Base mBART    | Fine-tuned mBART     |
| ----------------- | ------------- | -------------------- |
| **Accuracy**      | ~75%          | **~95%**             |
| **Training Time** | -             | 30-45 min (T4 GPU)   |
| **Dataset Size**  | 50+ languages | 1,101 sentence pairs |
| **Parameters**    | 610M (frozen) | 610M (fine-tuned)    |

---

## ‚ú® Features

- **üöÄ Production-Ready**: Clean, modular code with comprehensive error handling
- **üìä Educational**: Detailed comments explaining each step of the process
- **‚ö° Efficient**: Optimized for Google Colab free tier (T4 GPU)
- **üîÑ Reproducible**: Fixed random seeds and deterministic training
- **üìà Well-Documented**: Extensive inline documentation and examples
- **üéØ Practical**: Real-world translation quality on Telugu language pair

---

## üéì Inspiration

This implementation was inspired by **[Sai Rohith Vulapu's](https://www.linkedin.com/in/sai-rohith-vulapu)** insightful exploration of transformer architectures and the fundamental question:

> _"How much of LLM performance comes from architecture, and how much from the data it was trained on?"_

His hands-on journey‚Äîbuilding a transformer from scratch (~75% accuracy) and then leveraging mBART fine-tuning (~95% accuracy)‚Äîdemonstrates a crucial insight for machine learning practitioners:

**Architecture matters, but data and pre-training are the true game changers.**

Based on empirical observations:

- **Data & Pre-training**: ~60-70% of LLM performance
- **Architecture Design**: ~30-40% of LLM performance

This repository builds upon that learning by providing a production-ready implementation of the fine-tuning approach that achieved superior results.

üìù **Read the full architectural deep-dive**: [Sai Rohith's Blog Post](https://www.linkedin.com/posts/vijenderp_transformers-llm-finetuning-activity-7374652231066955776-kkkv)

---

## üõ†Ô∏è Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-capable GPU (recommended) or CPU
- 8GB+ RAM (12GB+ recommended)

### Option 1: Google Colab (Recommended for Beginners)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)

1. Open the notebook in Google Colab
2. Enable GPU: `Runtime` ‚Üí `Change runtime type` ‚Üí `T4 GPU`
3. Run all cells sequentially

### Option 2: Local Installation

```bash
# Clone the repository
git clone https://github.com/your-org/english-telugu-translation.git
cd english-telugu-translation

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

**requirements.txt:**

```txt
transformers>=4.35.0
datasets>=2.14.0
torch>=2.0.0
pandas>=2.0.0
scikit-learn>=1.3.0
accelerate>=0.24.0
huggingface_hub>=0.19.0
```

---

## üöÄ Quick Start

### 1. Prepare Your Environment

```python
import os
os.environ["WANDB_DISABLED"] = "true"  # Disable Weights & Biases logging

import torch
print(f"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}")
```

### 2. Load and Prepare Data

```python
from datasets import Dataset
import pandas as pd

# Load Telugu-English parallel corpus
df = pd.read_parquet("hf://datasets/Shreya3095/TeluguTranslator/data/train-00000-of-00001.parquet")

# Clean and split
train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)
train_dataset = Dataset.from_pandas(train_df[['english', 'telugu']])
```

### 3. Fine-tune mBART

```python
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

# Load pre-trained model
model = MBartForConditionalGeneration.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")
tokenizer = MBart50TokenizerFast.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")

# Configure for English‚ÜíTelugu
tokenizer.src_lang = "en_XX"
tokenizer.tgt_lang = "te_IN"

# Train (see notebook for complete training loop)
trainer.train()
```

### 4. Translate

```python
def translate(text):
    inputs = tokenizer(text, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id["te_IN"])
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Test
print(translate("Hello, how are you?"))
# Output: ‡∞π‡∞≤‡±ã, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?
```

---

## üìö Documentation

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         mBART-50 Architecture           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Encoder (12 layers)                    ‚îÇ
‚îÇ    ‚îú‚îÄ Self-Attention                    ‚îÇ
‚îÇ    ‚îú‚îÄ Feed-Forward Network              ‚îÇ
‚îÇ    ‚îî‚îÄ Layer Normalization               ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Decoder (12 layers)                    ‚îÇ
‚îÇ    ‚îú‚îÄ Masked Self-Attention             ‚îÇ
‚îÇ    ‚îú‚îÄ Cross-Attention                   ‚îÇ
‚îÇ    ‚îú‚îÄ Feed-Forward Network              ‚îÇ
‚îÇ    ‚îî‚îÄ Layer Normalization               ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Language Model Head                    ‚îÇ
‚îÇ    ‚îî‚îÄ Projects to 250,054 tokens        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Training Pipeline

```
1. Data Loading
   ‚îî‚îÄ Load Telugu-English parallel corpus (1,101 pairs)

2. Preprocessing
   ‚îú‚îÄ Tokenization (max_length=128)
   ‚îú‚îÄ Padding & Truncation
   ‚îî‚îÄ Label creation

3. Fine-tuning
   ‚îú‚îÄ Batch Size: 8
   ‚îú‚îÄ Learning Rate: 5e-5
   ‚îú‚îÄ Epochs: 10
   ‚îú‚îÄ Optimizer: AdamW
   ‚îî‚îÄ Loss: CrossEntropyLoss with label smoothing

4. Evaluation
   ‚îî‚îÄ Beam Search (num_beams=5)
```

### Hyperparameters Explained

| Parameter       | Value | Why?                                                                   |
| --------------- | ----- | ---------------------------------------------------------------------- |
| `learning_rate` | 5e-5  | Standard for transformer fine-tuning; prevents catastrophic forgetting |
| `batch_size`    | 8     | Balances GPU memory (T4: 16GB) with training speed                     |
| `num_epochs`    | 10    | Sufficient for convergence on 1K samples without overfitting           |
| `max_length`    | 128   | Covers 95%+ of sentence lengths in the dataset                         |
| `weight_decay`  | 0.01  | Light regularization to prevent overfitting                            |
| `num_beams`     | 5     | Beam search width for higher translation quality                       |
| `fp16`          | True  | Mixed precision training for 2x speedup on modern GPUs                 |

---

## üîß Troubleshooting

### Common Issues and Solutions

#### 1. **Out of Memory (OOM) Error**

**Symptom:**

```
RuntimeError: CUDA out of memory. Tried to allocate X GB
```

**Solutions:**

```python
# Option A: Reduce batch size
per_device_train_batch_size=4  # Down from 8

# Option B: Enable gradient accumulation
gradient_accumulation_steps=2  # Simulates batch_size=16 with memory of batch_size=8

# Option C: Use CPU (slower but works)
device = "cpu"
```

#### 2. **Slow Training on CPU**

**Symptom:** Training takes >2 hours

**Solutions:**

```python
# Enable threading
torch.set_num_threads(4)

# Reduce epochs for testing
num_train_epochs=3

# Use smaller model (if accuracy isn't critical)
model_name = "facebook/mbart-large-cc25"  # 25 languages instead of 50
```

#### 3. **Low Translation Quality**

**Symptom:** Translations are grammatically incorrect

**Root Causes & Fixes:**

| Issue                 | Solution                                                             |
| --------------------- | -------------------------------------------------------------------- |
| Insufficient training | Increase `num_train_epochs` to 15-20                                 |
| Poor data quality     | Clean dataset: remove duplicates, fix encoding issues                |
| Wrong language codes  | Verify `tokenizer.src_lang="en_XX"` and `tokenizer.tgt_lang="te_IN"` |
| Overfitting           | Add dropout: `model.config.dropout=0.3`                              |

#### 4. **"Connection Reset" During Model Download**

**Symptom:**

```
ConnectionResetError: [Errno 104] Connection reset by peer
```

**Solution:**

```python
# Use mirror or local cache
from huggingface_hub import snapshot_download

snapshot_download("facebook/mbart-large-50-many-to-many-mmt",
                  cache_dir="./models",
                  resume_download=True)
```

#### 5. **Tokenizer Warnings**

**Symptom:**

```
Token indices sequence length is longer than the specified maximum sequence length
```

**Solution:**

```python
# Ensure truncation is enabled
tokenizer(text, max_length=128, truncation=True, padding="max_length")
```

---

## ‚ö° Performance Optimization

### Speed Improvements

#### 1. **Enable Mixed Precision Training**

```python
training_args = Seq2SeqTrainingArguments(
    fp16=True,  # 2x faster on T4/V100/A100 GPUs
    fp16_full_eval=True
)
```

#### 2. **Use DataLoader Optimizations**

```python
training_args = Seq2SeqTrainingArguments(
    dataloader_num_workers=4,  # Parallel data loading
    dataloader_pin_memory=True  # Faster GPU transfer
)
```

#### 3. **Gradient Checkpointing** (for limited memory)

```python
model.gradient_checkpointing_enable()  # Trade speed for memory
```

### Memory Optimizations

```python
# For 8GB GPU (e.g., Colab T4)
per_device_train_batch_size=4
gradient_accumulation_steps=2
fp16=True

# For 16GB GPU (e.g., Colab Pro V100)
per_device_train_batch_size=8
gradient_accumulation_steps=1
fp16=True
```

---

## üìä Evaluation Metrics

### Automatic Metrics

```python
from datasets import load_metric

# BLEU Score
bleu = load_metric("bleu")
predictions = [translate(ex) for ex in test_data]
bleu_score = bleu.compute(predictions=predictions, references=references)
print(f"BLEU: {bleu_score['bleu']:.2f}")

# Expected range: 0.35-0.50 for this dataset size
```

### Manual Evaluation

```python
test_cases = [
    ("Hello", "‡∞π‡∞≤‡±ã"),
    ("Good morning", "‡∞∂‡±Å‡∞≠‡±ã‡∞¶‡∞Ø‡∞Ç"),
    ("Thank you", "‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å")
]

for english, expected_telugu in test_cases:
    predicted = translate(english)
    print(f"EN: {english}")
    print(f"Expected: {expected_telugu}")
    print(f"Predicted: {predicted}")
    print(f"Match: {predicted == expected_telugu}\n")
```

---

## üéØ Model Deployment

### Save and Load Model

```python
# Save fine-tuned model
model.save_pretrained("./mbart-en-te-finetuned")
tokenizer.save_pretrained("./mbart-en-te-finetuned")

# Load for inference
from transformers import pipeline
translator = pipeline("translation", model="./mbart-en-te-finetuned")
translator("Hello world")
```

### Deploy as API (FastAPI)

```python
# app.py
from fastapi import FastAPI
from transformers import pipeline

app = FastAPI()
translator = pipeline("translation", model="./mbart-en-te-finetuned")

@app.post("/translate")
def translate(text: str):
    return {"translation": translator(text)[0]['translation_text']}

# Run: uvicorn app:app --reload
```

### Hugging Face Hub Deployment

```python
# Login
!huggingface-cli login

# Push to Hub
model.push_to_hub("your-username/mbart-en-te")
tokenizer.push_to_hub("your-username/mbart-en-te")

# Use from anywhere
translator = pipeline("translation", model="your-username/mbart-en-te")
```

---

## üìÇ Project Structure

```
english-telugu-translation/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ english_to_telugu_translation.ipynb  # Main training notebook
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ train.csv                            # Training data (auto-generated)
‚îÇ   ‚îî‚îÄ‚îÄ val.csv                              # Validation data (auto-generated)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ mbart-finetuned-en-te/              # Saved model checkpoints
‚îú‚îÄ‚îÄ results/                                 # Training logs and metrics
‚îú‚îÄ‚îÄ requirements.txt                         # Python dependencies
‚îú‚îÄ‚îÄ README.md                               # This file
‚îî‚îÄ‚îÄ LICENSE                                 # MIT License
```

---

## üß™ Testing

### Run Unit Tests

```python
def test_translation_quality():
    test_cases = {
        "Hello": "‡∞π‡∞≤‡±ã",
        "Thank you": "‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å",
        "Good morning": "‡∞∂‡±Å‡∞≠‡±ã‡∞¶‡∞Ø‡∞Ç"
    }

    for english, expected in test_cases.items():
        result = translate(english)
        assert result == expected, f"Failed: {english} -> {result} (expected {expected})"

test_translation_quality()
print("‚úÖ All tests passed!")
```

---

## üî¨ Advanced Usage

### Custom Dataset

```python
# Use your own parallel corpus
custom_data = pd.DataFrame({
    'english': ['sentence 1', 'sentence 2'],
    'telugu': ['‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞Ç 1', '‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞Ç 2']
})

custom_dataset = Dataset.from_pandas(custom_data)
```

### Bidirectional Translation (Telugu‚ÜíEnglish)

```python
# Simply swap source/target languages
tokenizer.src_lang = "te_IN"
tokenizer.tgt_lang = "en_XX"

# Retrain with same pipeline
trainer.train()
```

### Multi-GPU Training

```python
training_args = Seq2SeqTrainingArguments(
    per_device_train_batch_size=8,
    n_gpu=4,  # Use 4 GPUs
    # Effective batch size: 8 * 4 = 32
)
```

---

## üìà Results Comparison

### Sample Translations

| Input (English)     | Base mBART             | Fine-tuned mBART           | Human Reference           |
| ------------------- | ---------------------- | -------------------------- | ------------------------- |
| Hello, how are you? | ‡∞π‡∞≤‡±ã, ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞µ‡±ç       | **‡∞π‡∞≤‡±ã, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?** | ‡∞π‡∞≤‡±ã, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å? ‚úÖ |
| Good morning        | ‡∞â‡∞¶‡∞Ø‡∞Ç ‡∞Æ‡∞Ç‡∞ö‡∞ø‡∞¶‡∞ø            | **‡∞∂‡±Å‡∞≠‡±ã‡∞¶‡∞Ø‡∞Ç**                | ‡∞∂‡±Å‡∞≠‡±ã‡∞¶‡∞Ø‡∞Ç ‚úÖ                |
| Thank you very much | ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å        | **‡∞ö‡∞æ‡∞≤‡∞æ ‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å**        | ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å ‚úÖ        |
| I went to school    | ‡∞®‡±á‡∞®‡±Å ‡∞™‡∞æ‡∞†‡∞∂‡∞æ‡∞≤‡∞ï‡±Å ‡∞µ‡±Ü‡∞≥‡±ç‡∞≥‡∞æ‡∞®‡±Å | **‡∞®‡±á‡∞®‡±Å ‡∞™‡∞æ‡∞†‡∞∂‡∞æ‡∞≤‡∞ï‡±Å ‡∞µ‡±Ü‡∞≥‡±ç‡∞≤‡∞æ‡∞®‡±Å** | ‡∞®‡±á‡∞®‡±Å ‡∞¨‡∞°‡∞ø‡∞ï‡∞ø ‡∞µ‡±Ü‡∞≥‡±ç‡∞≥‡∞æ‡∞®‡±Å ‚úÖ    |

### Training Curves

```
Epoch | Train Loss | Val Loss | Accuracy
------|------------|----------|----------
1     | 2.667      | 0.160    | 78%
3     | 0.064      | 0.132    | 89%
5     | 0.010      | 0.133    | 93%
10    | 0.004      | 0.135    | 95%
```

---

## ü§ù Contributing

We welcome contributions! Please follow these guidelines:

1. **Fork the repository**
2. **Create a feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit your changes**: `git commit -m 'Add amazing feature'`
4. **Push to branch**: `git push origin feature/amazing-feature`
5. **Open a Pull Request**

### Development Setup

```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Format code
black .
isort .
```

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **[Sai Rohith Vulapu](https://www.linkedin.com/in/sai-rohith-vulapu/)** for the original inspiration and architectural insights
- **Facebook AI Research** for the mBART pre-trained model
- **Hugging Face** for the Transformers library
- **Shreya3095** for the Telugu translation dataset

---
<!-- 
## üìû Support

- **Issues**: [GitHub Issues](https://github.com/your-org/english-telugu-translation/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-org/english-telugu-translation/discussions)
- **Email**: support@your-org.com

--- -->

## üîó Related Resources

- [Original Blog Post by Sai Rohith Vulapu](https://www.linkedin.com/posts/vijenderp_transformers-llm-finetuning-activity-7374652231066955776-kkkv)
- [mBART Paper (Liu et al., 2020)](https://arxiv.org/abs/2001.08210)
- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers)
- [Telugu NLP Resources](https://github.com/telugu-nlp)

---

<div align="center">

**Made with ‚ù§Ô∏è for the Telugu NLP community**

‚≠ê **Star this repo if you find it helpful!**
